import streamlit as st

st.set_page_config(
    page_title="KíŒ ë°ëª¬ í—Œí„°ìŠ¤ íŒ¬ë¤ ë¶„ì„",
    layout="wide"
)

st.title("KíŒ ë°ëª¬ í—Œí„°ìŠ¤ íŒ¬ë¤ í˜•ì„± ìš”ì¸ ë¶„ì„")
st.write("í•™ë²ˆ: C321017    ì´ë¦„: ê¹€ì—°í˜¸")

st.divider()

st.sidebar.header("ğŸ” ë¶„ì„ ì˜µì…˜")

date_range = st.sidebar.date_input(
    "ë¶„ì„ ê¸°ê°„ ì„ íƒ",
    []
)

top_n = st.sidebar.slider(
    "í‚¤ì›Œë“œ ê°œìˆ˜ ì„ íƒ",
    min_value=10,
    max_value=50,
    value=30,
    step=5
)

show_global = st.sidebar.checkbox(
    "ê¸€ë¡œë²Œ ì„±ê³¼ í‚¤ì›Œë“œ í¬í•¨",
    value=True
)

wc_max_words = st.sidebar.slider(
    "ì›Œë“œí´ë¼ìš°ë“œ ìµœëŒ€ ë‹¨ì–´ ìˆ˜",
    min_value=50,
    max_value=300,
    value=150,
    step=10
)

min_edge = st.sidebar.slider(
    "ë„¤íŠ¸ì›Œí¬ ìµœì†Œ ì—°ê²° ë¹ˆë„",
    min_value=1,
    max_value=10,
    value=3
)

st.header("1ï¸âƒ£ Seabornì„ ì´ìš©í•œ ì‹œì ë³„ ê¸°ì‚¬ ìˆ˜ ì¶”ì´ ë¶„ì„")
st.write(
    "ì¼€ì´íŒ ë°ëª¬ í—Œí„°ìŠ¤ ê´€ë ¨ ì˜¨ë¼ì¸ ê¸°ì‚¬ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì´ìŠˆê°€ ì–´ë–¤ ì‹œì ë¶€í„° ì§‘ì¤‘ì ìœ¼ë¡œ í™•ì‚°ë˜ì—ˆëŠ”ì§€ë¥¼ í™•ì¸í•˜ëŠ” Seaborn ê·¸ë˜í”„. (koreanized-matplotlibê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ ì˜ì–´ë¡œ ì œëª© ë“±ì„ ì‘ì„±í•˜ì˜€ìŠµë‹ˆë‹¤. seaborn ê·¸ë˜í”„ ì½”ë“œì—ì„œ ë‚ ì§œ ë²”ìœ„ í•„í„°ë§ ì½”ë“œëŠ” AIë¥¼ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤.)"
)

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

df = pd.read_csv("F:/Users/ê¹€ ì—°í˜¸/Desktop/í•™êµ/2025-2/ë°ì´í„°ì‹œê°í™”/data_ë°ì´í„°ì‹œê°í™”/ì¼€ë°í—Œ.csv")
df["pubDate"] = pd.to_datetime(df["pubDate"]).dt.date

if len(date_range) == 2:
    df = df[
        (df["pubDate"] >= date_range[0]) &
        (df["pubDate"] <= date_range[1])
    ]

date_count = (
    df.groupby("pubDate")
    .size()
    .reset_index(name="count")
)

fig, ax = plt.subplots(figsize=(10, 4))
sns.lineplot(data=date_count, x="pubDate", y="count", marker="o", ax=ax)
ax.set_xlabel("date")
ax.set_ylabel("number of articles")
ax.set_title("Trend of K-Pop Demon Hunters Article Counts Over Time")

st.pyplot(fig)

st.write(
    "ìœ„ì˜ Seaborn ê·¸ë˜í”„ëŠ” ì¼€ì´íŒ ë°ëª¬ í—Œí„°ìŠ¤ì™€ ê´€ë ¨ëœ ê¸°ì‚¬ ìˆ˜ê°€ ë‚ ì§œë³„ë¡œ ì–´ë–»ê²Œ ë³€í™”í–ˆëŠ”ì§€ë¥¼ ë³´ì—¬ì¤€ë‹¤. 11ì›” ë§ë¶€í„° ê¸°ì‚¬ ìˆ˜ê°€ ì ì°¨ ì¦ê°€í•˜ë©° ì‘í’ˆì— ëŒ€í•œ ê´€ì‹¬ì´ ì»¤ì§€ê³  ìˆìŒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. ì¤‘ê°„ì— ê¸°ì‚¬ ìˆ˜ê°€ ì¼ì‹œì ìœ¼ë¡œ ê°ì†Œí•˜ëŠ” êµ¬ê°„ë„ ë‚˜íƒ€ë‚˜ì§€ë§Œ, ì „ì²´ì ìœ¼ë¡œ ë³´ë©´ 12ì›” ì¤‘ìˆœê¹Œì§€ëŠ” ì¦ê°ì„ ë°˜ë³µí•˜ë©´ì„œë„ ì¦ê°€í•˜ëŠ” ì¶”ì„¸ë¥¼ ë³´ì¸ë‹¤. ì´ë¥¼ í†µí•´ ì¼€ì´íŒ ë°ëª¬ í—Œí„°ìŠ¤ì— ëŒ€í•œ ê´€ì‹¬ì´ ì ì  ì»¤ì¡ŒìŒì„ ì•Œ ìˆ˜ ìˆë‹¤."
)


st.divider()

st.header("2ï¸âƒ£ WordCloudë¥¼ ì´ìš©í•œ ë‹¨ì–´ ë¹ˆë„ ì‹œê°í™” ë¶„ì„")
st.write(
    "ì¼€ì´íŒ ë°ëª¬ í—Œí„°ìŠ¤ ê´€ë ¨ ì˜¨ë¼ì¸ ê¸°ì‚¬ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê¸°ì‚¬ ì œëª©ê³¼ ë³¸ë¬¸ì—ì„œ ìì£¼ ë“±ì¥í•œ í‚¤ì›Œë“œë¥¼ ì‹œê°í™”í•œ WordCloud ê·¸ë˜í”„. 'ì¼€ì´íŒ ë°ëª¬ í—Œí„°ìŠ¤', 'ì¼€ë°í—Œ' ë“± ê³ ìœ ëª…ì‚¬ì™€ ë¶ˆìš©ì–´ ì œê±° ë° í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ë¥¼ í†µí•´ ì˜ë¯¸ ìˆëŠ” ë‹¨ì–´ë§Œì„ ì¶”ì¶œí•˜ì˜€ìœ¼ë©°, í•œê¸€ í‘œí˜„ì„ ìœ„í•´ WordCloudì— í•œê¸€ í°íŠ¸, ë‚˜ëˆ”ê³ ë”• í°íŠ¸ë¥¼ ì ìš©í•˜ì˜€ë‹¤. WordCloud ìƒì„± ë° ì „ì²˜ë¦¬ ê³¼ì •ì€ ê°•ì˜ë¡ì˜ í…ìŠ¤íŠ¸ ì‹œê°í™” ì½”ë“œ íë¦„ì„ ê¸°ë°˜ìœ¼ë¡œ êµ¬í˜„í•˜ì˜€ë‹¤.(í…ìŠ¤íŠ¸ ì •í™”, ë¶ˆìš©ì–´ë“¤ì€ AIë¡œ ì‘ì„±í•˜ê³  ë¶„ì„ ëª©ì ì— ë§ê²Œ í¸ì§‘í•˜ì˜€ìŠµë‹ˆë‹¤.)"
)

import pandas as pd
import re
from wordcloud import WordCloud, STOPWORDS

df["title"] = df["title"].fillna("").astype(str)
df["description"] = df["description"].fillna("").astype(str)
df["text"] = (df["title"] + " " + df["description"]).str.strip()

text_kdh = " ".join(df["text"].tolist())

text_kdh_clean = text_kdh
text_kdh_clean = re.sub(r"&quot;|quot", " ", text_kdh_clean)
text_kdh_clean = re.sub(r"&lt;|lt", " ", text_kdh_clean)
text_kdh_clean = re.sub(r"&gt;|gt", " ", text_kdh_clean)
text_kdh_clean = re.sub(r"&amp;|amp", " ", text_kdh_clean)
text_kdh_clean = re.sub(r"[\'\"â€œâ€â€˜â€™]", " ", text_kdh_clean)
text_kdh_clean = re.sub(r"[^0-9A-Za-zê°€-í£\s]", " ", text_kdh_clean)
text_kdh_clean = re.sub(r"\s+", " ", text_kdh_clean).strip()

stop_words_kdh = [
    # 1. ì£¼ì œì–´ ë° ê³ ìœ ëª…ì‚¬ (ë¶„ì„ ëª©ì ì— ë”°ë¼ ì œê±°)
    'ë°ëª¬', 'í—Œí„°ìŠ¤', 'ì¼€ì´íŒ', 'k', 'K', 'KíŒ', 'ì¼€ë°í—Œ', 'íŒ', 'ì• ë‹ˆë©”ì´ì…˜', 'ë„·í”Œë¦­ìŠ¤', 
    'ì˜í™”', 'kíŒ', 'ost', 'demon', 'hunters', 'ê±¸ê·¸ë£¹', 'ì•„ì´ëŒ', 'ì†Œë‹¤', 
    'ê³¨ë“ ', 'ê³¨ë“ ê¸€ë¡œë¸Œ', 'ë¹Œë³´ë“œ', 'ë°•ì°¬ìš±', 'ê°•', 'ë§¤ê¸°',
    
    # 2. ì¡°ì‚¬, ì ‘ì†ì‚¬ ë° ì–´ë¯¸ (í•œêµ­ì–´ ì¼ë°˜ ë¶ˆìš©ì–´)
    'ì˜', 'ê°€', 'ì´', 'ì™€', 'ê³¼', 'ì—', 'ë¥¼', 'ì„', 'ëŠ”', 'ì€', 'í•œ', 'ìˆëŠ”', 'ì—†ë‹¤',
    
    # 3. ë…¸ì´ì¦ˆ ë° ì¼ë°˜ ë¹ˆì¶œ ë‹¨ì–´
    'quot', 'ë“±', 'ìœ„', 'ì¼', 'ì „', 'ë…„', 'ê°œ', 'ìµœê³ ', 'íŠ¹íˆ', 'ì»¬ì²˜', 'ë¶€ë¬¸'
]

STOPWORDS.update(stop_words_kdh)

words_list = text_kdh_clean.split()
words_list = [w for w in words_list if w not in STOPWORDS]
text_kdh_clean2 = " ".join(words_list)

#ì´ë¯¸ì§€íŒŒì¼ì„ ë¶ˆëŸ¬ì™€ ndarrayë¡œ ë³€í™˜
import numpy as np
from PIL import Image

#ë§ˆìŠ¤í¬ê°€ ë  ì´ë¯¸ì§€ ë¶ˆëŸ¬ì˜¤ê¸°
image = Image.open(
    "F:/Users/ê¹€ ì—°í˜¸/Desktop/í•™êµ/2025-2/ë°ì´í„°ì‹œê°í™”/data_ë°ì´í„°ì‹œê°í™”/cross_new.png" # ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ
    ).resize(size=(800, 800)) #ì´ë¯¸ì§€ í¬ê¸° ì§€ì •
wc_mask=Image.new("RGB", image.size, (255,255,255))
wc_mask.paste(im=image, mask=image)
wc_mask = np.array(wc_mask)

han_font_path = "F:/Users/ê¹€ ì—°í˜¸/Downloads/nanum-all_new/ë‚˜ëˆ” ê¸€ê¼´/ë‚˜ëˆ”ê³ ë”•/NanumFontSetup_OTF_GOTHIC/NanumGothic.otf"

def showWordCloudBasic(wc):
    fig = plt.figure(figsize=(8, 5))
    plt.imshow(wc)
    plt.axis("off")
    return fig

words_kdh = WordCloud(
    font_path=han_font_path,
    max_words=wc_max_words,
    stopwords=STOPWORDS,
    background_color="black",
    mask=wc_mask,
    colormap="coolwarm"
).generate(text_kdh_clean2)

fig = showWordCloudBasic(words_kdh)
st.pyplot(fig)

st.write(
    "ìœ„ì˜ WordCloud ì‹œê°í™” ê²°ê³¼ë¥¼ ë³´ë©´ ê°€ì¥ í¬ê²Œ 'ê¸€ë¡œë²Œ'ì´ë¼ëŠ” ë‹¨ì–´ë¥¼ í™•ì¸í•  ìˆ˜ ìˆë‹¤. ì´ëŠ” ì¼€ì´íŒë°ëª¬í—Œí„°ìŠ¤ê°€ êµ­ë‚´ ë¿ ì•„ë‹ˆë¼ í•´ì™¸ì—ì„œë„ ì—„ì²­ë‚œ ì—´í’ì´ë¼ëŠ” ê²ƒê³¼ ê¸€ë¡œë²Œ ì´ìŠˆë¼ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤. ë˜ Golden, Soda Pop ë“± ì¼€ì´íŒ ë°ëª¬ í—Œí„°ìŠ¤ì˜ ë…¸ë˜ ì œëª©ë“¤ë„ ë§¤ìš° í° ê¸€ì”¨ì¸ ê²ƒì„ ë³´ì•„ ì¼€ë°í—Œì˜ ì¸ê¸°ì— ë…¸ë˜ê°€ ì¤‘ìš”í•œ ì—­í• ì„ í–ˆë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ ì¼€ì´íŒ ë°ëª¬ í—Œí„°ìŠ¤ëŠ” ì‘í’ˆ ìì²´ì˜ ì¬ë¯¸ë¿ ì•„ë‹ˆë¼ ìŒì•…ì ì¸ ì¸¡ë©´ì—ì„œë„ ë†’ì€ ì™„ì„±ë„ì™€ ëŒ€ì¤‘ì„±ì„ ê°–ì¶”ì—ˆë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤."
)


st.divider()

st.header("3ï¸âƒ£ Altairë¥¼ ì´ìš©í•œ í‚¤ì›Œë“œ ë¹ˆë„ ë¶„ì„")
st.write(
    "ì•ì„œ WordCloud ì—ì„œ ì „ì²˜ë¦¬ í•˜ì˜€ë˜ ê¸°ì‚¬ ë°ì´í„°ì—ì„œ í‚¤ì›Œë“œ ë¹ˆë„ë¥¼ ì§‘ê³„í•˜ì—¬ í‚¤ì›Œë“œì˜ ë¹ˆë„ë¥¼ Altair ë§‰ëŒ€ê·¸ë˜í”„ë¡œ ì‹œê°í™”í•˜ì˜€ë‹¤. ê·¸ë˜í”„ì˜ ê°€ë…ì„±ì„ ìœ„í•´ ê°€ë¡œ ë§‰ëŒ€ ê·¸ë˜í”„ë¡œ ì‹œê°í™”í•˜ì˜€ê³ ,  ë¹ˆë„ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ í•œëˆˆì— í™•ì¸í•  ìˆ˜ ìˆë„ë¡ í•˜ì˜€ë‹¤. (altair ê·¸ë˜í”„ì˜ ì˜¤ë¥˜ë¥¼ AIì˜ ë„ì›€ì„ ë°›ì•„ í•´ê²°í•˜ì˜€ìŠµë‹ˆë‹¤.)"
)
import pandas as pd
import altair as alt

kw_df = (
    pd.Series(text_kdh_clean2.split())
    .value_counts()
    .head(top_n)
    .reset_index()
)
kw_df.columns = ["keyword", "count"]

c = (
    alt.Chart(kw_df)
    .mark_bar()
    .encode(
        x="count",
        y=alt.Y("keyword", sort="-x"),
        tooltip=["keyword", "count"]
    )
)

st.altair_chart(c, use_container_width=True)

st.write(
    "ìœ„ì˜ Altair ë§‰ëŒ€ê·¸ë˜í”„ë¥¼ í†µí•´ ì¼€ì´íŒ ë°ëª¬ í—Œí„°ìŠ¤ ê´€ë ¨ ì˜¨ë¼ì¸ ê¸°ì‚¬ì—ì„œ ë‚˜ì˜¤ëŠ” í‚¤ì›Œë“œë“¤ì˜ ë¹ˆë„ë¥¼ í™•ì¸í•  ìˆ˜ ìˆë‹¤. ì•ì„œ WordCloudì˜ ê²°ê³¼ í•´ì„ê³¼ ë§ˆì°¬ê°€ì§€ë¡œ, ê¸€ë¡œë²Œì´ ê°€ì¥ í° ë¹ˆë„ë¥¼ ë³´ì˜€ë‹¤. WordCloudì˜ ì‹œê°í™”ì™€ ë‹¤ë¥¸ ì ì€ í‚¤ì›Œë“œë“¤ê°„ì˜ ë¹„êµê°€ í›¨ì”¬ ë” ì‰½ë‹¤ëŠ” ê²ƒì´ë‹¤. WordCloudëŠ” í‚¤ì›Œë“œì˜ ë¹ˆë„ìˆ˜ë‚˜ í‚¤ì›Œë“œ ê°„ì— ë¬´ì—‡ì´ ë” í°ì§€ ì•Œê¸°ê°€ ì‰½ì§€ ì•Šì§€ë§Œ, Altair ê·¸ë˜í”„ëŠ” ë§‰ëŒ€ë¡œ ëŒ€ëµì˜ ìˆ˜ì¹˜ì™€ í•¨ê»˜ ì‰½ê²Œ ì•Œ ìˆ˜ ìˆë‹¤.  ê·¸ë˜í”„ë¡œ ì¼€ì´íŒ ë°ëª¬ í—Œí„°ìŠ¤ëŠ” ì‘í’ˆì„±ê³¼ ìŒì•…ì„±ì„ ëª¨ë‘ ê°–ì¶”ì—ˆë‹¤ëŠ” ê²ƒì„ ë‹¤ì‹œ í•œ ë²ˆ í™•ì¸í•  ìˆ˜ ìˆì—ˆë‹¤."
)   

st.divider()

st.header("4ï¸âƒ£ Networkxë¥¼ ì´ìš©í•œ í‚¤ì›Œë“œ ë„¤íŠ¸ì›Œí¬ êµ¬ì¡° ë¶„ì„")
st.write(
    "í‚¤ì›Œë“œ ê°„ ë™ì‹œ ì¶œí˜„ ê´€ê³„ë¥¼ Networkxë¥¼ ì´ìš©í•˜ì—¬ ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°ë¡œ ì‹œê°í™”í•˜ì˜€ë‹¤. ê° í‚¤ì›Œë“œëŠ” ë…¸ë“œë¥¼ ì˜ë¯¸í•˜ê³  í‚¤ì›Œë“œ ìŒì€ ì—£ì§€ë¡œ ì´ì–´ì§„ë‹¤.(koreanized-matplotlibê°€ ì•ˆ ë˜ì–´ AIë¥¼ ì‚¬ìš©í•˜ì˜€ìœ¼ë‚˜ í•œê¸€ë¡œ ì„¤ì •í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.)"
)
import re
import itertools
from collections import Counter
import numpy as np
import networkx as nx
import matplotlib.pyplot as plt

top_n_net = st.sidebar.slider("ë„¤íŠ¸ì›Œí¬ Top N í‚¤ì›Œë“œ", 10, 60, 30)
min_edge = st.sidebar.slider("ìµœì†Œ ë™ì‹œì¶œí˜„(ì—£ì§€ ê°€ì¤‘ì¹˜) ê¸°ì¤€", 1, 10, 2)

def clean_tokens(s):
    s = str(s)
    s = re.sub(r"&quot;|quot", " ", s)
    s = re.sub(r"&lt;|lt", " ", s)
    s = re.sub(r"&gt;|gt", " ", s)
    s = re.sub(r"&amp;|amp", " ", s)
    s = re.sub(r"[\'\"â€œâ€â€˜â€™]", " ", s)
    s = re.sub(r"[^0-9A-Za-zê°€-í£\s]", " ", s)
    s = re.sub(r"\s+", " ", s).strip()
    words = [w for w in s.split() if w and w not in STOPWORDS and len(w) >= 2]
    return words

docs_tokens = df["text"].fillna("").astype(str).apply(clean_tokens)

all_words = list(itertools.chain.from_iterable(docs_tokens.tolist()))
top_words = (
    pd.Series(all_words)
    .value_counts()
    .head(top_n_net)
    .index
    .tolist()
)

edges = Counter()

for tokens in docs_tokens:
    tokens = [t for t in tokens if t in top_words]
    tokens = list(dict.fromkeys(tokens))
    for node1, node2 in itertools.combinations(sorted(tokens), 2):
        edges[(node1, node2)] += 1

filtered_edges = {k: v for k, v in edges.items() if v >= min_edge}

G = nx.Graph()

weighted_edges = [
    (node1, node2, weight)
    for (node1, node2), weight in filtered_edges.items()
]
G.add_weighted_edges_from(weighted_edges)

if G.number_of_nodes() != 0:
    pos_spring = nx.spring_layout(
        G,
        k=0.3,
        iterations=50,
        seed=42
    )

    node_sizes = [G.degree(node) * 100 for node in G.nodes()]
    edge_widths = [G[u][v]["weight"] * 0.05 for u, v in G.edges()]

    plt.figure(figsize=(15, 15))
    nx.draw(
        G,
        pos_spring,
        with_labels=True,
        node_size=node_sizes,
        width=edge_widths,
        font_family=plt.rcParams["font.family"],
        font_size=12,
        node_color="skyblue",
        edge_color="gray",
        alpha=0.8
    )
    plt.title("Kpop demon hunters keywords network", size=20)
    plt.axis("off")
    st.pyplot(plt.gcf())
    plt.close()

st.write(
    "ì‹œê°í™”í•œ ê·¸ë˜í”„ë¥¼ ë³´ë©´ '2025', 'golden', 'ost' ë“±ì˜ ë‹¨ì–´ë“¤ì´ ì´ì–´ì ¸ìˆëŠ” ê²ƒìœ¼ë¡œ ë³´ì•„ ì˜¬í•´ ìŒì•…ì ìœ¼ë¡œ í° ì¸ê¸°ë¥¼ ëŒì—ˆìŒì„ ì•Œ ìˆ˜ ìˆë‹¤."
)


st.divider()

st.header("5ï¸âƒ£plotly ê·¸ë˜í”„ ì‹œê°í™”")

st.write(
    "í‚¤ì›Œë“œ ë“±ì¥ ë¹ˆë„ ìƒìœ„ í•­ëª©ì„ Plotly ë§‰ëŒ€ê·¸ë˜í”„ë¡œ ì‹œê°í™”í•˜ì˜€ë‹¤. Plotlyì˜ ì¸í„°ë™í‹°ë¸Œ ê¸°ëŠ¥ì„ í™œìš©í•˜ì—¬ ê° í‚¤ì›Œë“œì˜ ë¹ˆë„ë¥¼ ë§ˆìš°ìŠ¤ë¡œ í™•ì¸í•  ìˆ˜ ìˆë„ë¡ ì‹œê°í™”í•˜ì˜€ë‹¤. (plotly ë°” ì°¨íŠ¸ ì½”ë“œëŠ” AIì˜ ë„ì›€ì„ ë°›ì•„ ì‘ì„±í•˜ì˜€ìŠµë‹ˆë‹¤.)"
)

import pandas as pd
import plotly.express as px

kw_df_plotly = (
    pd.Series(text_kdh_clean2.split())
    .value_counts()
    .head(top_n)
    .reset_index()
)
kw_df_plotly.columns = ["keyword", "count"]

fig = px.bar(
    kw_df_plotly.sort_values("count", ascending=True),
    x="count",
    y="keyword",
    orientation="h",
    hover_data=["count"]
)

st.plotly_chart(fig, use_container_width=True)


st.write(
    "ì‹œê°í™” ê²°ê³¼ë¥¼ ë³´ë©´ ë¯¸êµ­ê³¼ í•œêµ­ê³¼ ê°™ì€ êµ­ê°€ í‚¤ì›Œë“œê°€ ê°€ì¥ ë†’ì€ ë¹ˆë„ë¥¼ ë³´ì¸ë‹¤. ì´ëŠ” êµ­ë‚´ì™€ í•´ì™¸ì—ì„œ ë™ì‹œì— ì£¼ëª©ì„ ë°›ê³  ìˆëŠ” ê²ƒì„ ë³´ì—¬ì¤€ë‹¤. ë˜í•œ ìŒì•…ê³¼ ê´€ë ¨ëœ í‚¤ì›Œë“œë“¤ì´ ìƒìœ„ì— ìˆëŠ” ê²ƒìœ¼ë¡œ ë³´ì•„ ìŒì•…ì´ íŒ¬ë¤ í˜•ì„±ì— ê¸°ì—¬í•˜ì˜€ìŒì„ ì•Œ ìˆ˜ ìˆë‹¤."
)

